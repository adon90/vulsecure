import scrapy
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from tutorial.items import ExploitItem
from scrapy.selector import HtmlXPathSelector
#from queries import *
import re


#count = 0
class exploitSpider(CrawlSpider):
    name = "exploitSpider"
    allowed_domains = ["www.exploit-db.com"]
    start_urls = [
        "https://www.exploit-db.com/search/?order_by=date&order=desc&pg="+str(i)+"&action=search" for i in range(1,2000)
    ]
  
    rules = [Rule(SgmlLinkExtractor(restrict_xpaths=('//td[@class="description"]/a')), callback='parse_item')]
    def parse_item(self, response):
        exploit = ExploitItem()
	
	#print "parsing........."
	
        cve_null = response.xpath("//table[@class='exploit_list']//tr[position()=1]/td[position()=2]/text()").extract()[0]  
        #print cve_null
        if cve_null.strip(" ") != "N/A":
        
            #print "CVE Detected!!!!!!!!!!"
            exploit['ID'] = response.xpath("//table[@class='exploit_list']//tr[position()=1]/td[position()=1]/text()").extract()[0]
            exploit['CVE']=response.xpath("//table[@class='exploit_list']//tr[position()=1]/td[position()=2]/a/text()").extract()[0]
            exploit['Date'] = response.xpath("//table[@class='exploit_list']//tr[position()=2]/td[position()=3]/text()").extract()[0]
            exploit['Exploit'] = response.xpath("//div[@id='container']/pre/text()").extract()[0]
            #print "CVE----------------->"+exploit['CVE']
            #print "ID----------------->"+exploit['ID']
            #print "Date----------------->"+exploit['Date']
            #print "Exploit----------------->"+exploit['Exploit']
            #insert_exploit(exploit['ID'],"CVE-"+exploit['CVE'],exploit['Date'],exploit['Exploit'])
				    
	  
